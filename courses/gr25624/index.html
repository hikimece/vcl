<!DOCTYPE html>
<html lang="en-us" dir="ltr" class="scroll-smooth" data-default-appearance="dark"
  data-auto-appearance="false"><head>
  <meta charset="utf-8" />
  
  <meta http-equiv="content-language" content="en-us" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title> &middot; VCL Lab.</title>
  <meta name="title" content=" &middot; VCL Lab." />
  
  
  
  
  
  <link rel="canonical" href="https://hikimece.github.io/vcl/courses/gr25624/" />
  
  <link rel="alternate" type="application/rss+xml" href="/vcl/courses/gr25624/index.xml" title="VCL Lab." />
  
  
  
  
  
  
  
  
  
  
  
  
  <link type="text/css" rel="stylesheet" href="/vcl/css/main.bundle.min.4c19904eea2898fc46b06fa5c55009ba1363d064960033f0f897529aa3fa8f0d92bd1677f3ba3998bdbae7703134f05b448c3708f2babb64da07c1798ef92dc2.css"
    integrity="sha512-TBmQTuoomPxGsG&#43;lxVAJuhNj0GSWADPw&#43;JdSmqP6jw2SvRZ387o5mL2653AxNPBbRIw3CPK6u2TaB8F5jvktwg==" />
  
  
  <script type="text/javascript" src="/vcl/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
  
  
  
  
  
  
  
  
  
  
  <script defer type="text/javascript" id="script-bundle" src="/vcl/js/main.bundle.min.c178288131a2f1ad46910438db47ac5f7e1c48cf949e49f6dc3310c8ec9660e23fe505805eba4e2e73711335808500360d773a2b64322feb35df52856edca286.js"
    integrity="sha512-wXgogTGi8a1GkQQ420esX34cSM&#43;Unkn23DMQyOyWYOI/5QWAXrpOLnNxEzWAhQA2DXc6K2QyL&#43;s131KFbtyihg==" data-copy="" data-copied=""></script>
  
  
  
  <script src="/vcl/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S&#43;Yti0U7QtuZvQ=="></script>
  
  
  
  <link rel="apple-touch-icon" sizes="180x180" href="/vcl/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/vcl/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/vcl/favicon-16x16.png" />
  <link rel="manifest" href="/vcl/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="https://hikimece.github.io/vcl/courses/gr25624/">
  <meta property="og:site_name" content="VCL Lab.">
  <meta property="og:title" content="VCL Lab.">
  <meta property="og:description" content="Paper Reading List # Weekly Topic Papers for Presentation Resource Deep Learning [Chen et al. 2020] A Simple Framework for Contrastive Learning of Visual Representations, ICML [Paper] [GitHub] [He et al. 2020] Momentum Contrast for Unsupervised Visual Representation Learning, CVPR [Paper] [GitHub] [Grill et al. 2020] Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning, NeurIPS [Paper] [GitHub] Convolutional Neural Networks (CNN) [Tan et al. 2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, ICML [Paper] [GitHub] [Liu et al. 2022] A ConvNet for the 2020s, CVPR [Paper] [GitHub] [Woo et al. 2023] ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders, CVPR [Paper] [GitHub] Transformer [Dosovitskiy et al. 2021] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR [Paper] [GitHub] [Caron et al. 2021] Emerging Properties in Self-Supervised Vision Transformers, ICCV [Paper] [GitHub] [He et al. 2022] Masked Autoencoders Are Scalable Vision Learners, CVPR [Paper] [GitHub] Overview of Generative AI [Isola et al. 2017] Image-to-Image Translation with Conditional Adversarial Networks, CVPR [Paper] [GitHub] [Devlin et al. 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL [Paper] [GitHub] [Dhariwal et al. 2021] Diffusion Models Beat GANs on Image Synthesis, NeurIPS [Paper] [GitHub] Generative Models [Mildenhall et al. 2020] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, ECCV [Paper] [GitHub] [Rombach et al. 2022] High-Resolution Image Synthesis with Latent Diffusion Models, CVPR [Paper] [GitHub] [Kerbl et al. 2023] 3D Gaussian Splatting for Real-time Radiance Field Rendering, SIGGRAPH [Paper] [GitHub] Large Language Moudel (LLM) [Chowdhery et al. 2022] PaLM: Scaling Language Modeling with Pathways, JMLR [Paper] [GitHub] [Wei et al. 2022] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS [Paper] [Ouyang et al. 2022] Training Language Models to Follow Instructions with Human Feedback, NeurIPS [Paper] [GitHub] Multimodal Learning [Radford et al. 2021] Learning Transferable Visual Models from Natural Language Supervision, ICML [Paper] [GitHub] [Radford et al. 2023] Robust Speech Recognition via Large-scale Weak Supervision, ICML [Paper] [GitHub] [Girdhar et al. 2023] ImageBind: One Embedding Space to Bind Them All, CVPR [Paper] [GitHub] Large Multimodal Model (LMM) [Alayrac et al. 2022] Flamingo: A Visual Language Model for Few-Shot Learning, NeurIPS [Paper] [GitHub] [Liu et al. 2023] Visual Instruction Tuning, NeurIPS [Paper] [GitHub] [Wu et al. 2024] VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks, NeurIPS [Paper] [GitHub] AI in Physical World [Driess et al. 2023] PaLM-E: An Embodied Multimodal Language Model, ICML [Paper] [GitHub] [Kim et al. 2024] OpenVLA: An Open-Source Vision-Language-Action Model, arXiv [Paper] [GitHub] [Wang et al. 2025] Magma: A Foundation Model for Multimodal AI Agents, CVPR [Paper] [GitHub] Ethics, Fairness, and AI Safety [Tao et al. 2024] When to Trust LLMs: Aligning Confidence with Response Quality, ACL [Paper] [GitHub] [Zhao et al. 2024] A Taxonomy of Challenges to Curating Fair Datasets, NeurIPS [Paper] [Li et al. 2025] T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation, CVPR [Paper] [GitHub]">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="VCL Lab.">
  <meta name="twitter:description" content="Paper Reading List # Weekly Topic Papers for Presentation Resource Deep Learning [Chen et al. 2020] A Simple Framework for Contrastive Learning of Visual Representations, ICML [Paper] [GitHub] [He et al. 2020] Momentum Contrast for Unsupervised Visual Representation Learning, CVPR [Paper] [GitHub] [Grill et al. 2020] Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning, NeurIPS [Paper] [GitHub] Convolutional Neural Networks (CNN) [Tan et al. 2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, ICML [Paper] [GitHub] [Liu et al. 2022] A ConvNet for the 2020s, CVPR [Paper] [GitHub] [Woo et al. 2023] ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders, CVPR [Paper] [GitHub] Transformer [Dosovitskiy et al. 2021] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR [Paper] [GitHub] [Caron et al. 2021] Emerging Properties in Self-Supervised Vision Transformers, ICCV [Paper] [GitHub] [He et al. 2022] Masked Autoencoders Are Scalable Vision Learners, CVPR [Paper] [GitHub] Overview of Generative AI [Isola et al. 2017] Image-to-Image Translation with Conditional Adversarial Networks, CVPR [Paper] [GitHub] [Devlin et al. 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL [Paper] [GitHub] [Dhariwal et al. 2021] Diffusion Models Beat GANs on Image Synthesis, NeurIPS [Paper] [GitHub] Generative Models [Mildenhall et al. 2020] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, ECCV [Paper] [GitHub] [Rombach et al. 2022] High-Resolution Image Synthesis with Latent Diffusion Models, CVPR [Paper] [GitHub] [Kerbl et al. 2023] 3D Gaussian Splatting for Real-time Radiance Field Rendering, SIGGRAPH [Paper] [GitHub] Large Language Moudel (LLM) [Chowdhery et al. 2022] PaLM: Scaling Language Modeling with Pathways, JMLR [Paper] [GitHub] [Wei et al. 2022] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS [Paper] [Ouyang et al. 2022] Training Language Models to Follow Instructions with Human Feedback, NeurIPS [Paper] [GitHub] Multimodal Learning [Radford et al. 2021] Learning Transferable Visual Models from Natural Language Supervision, ICML [Paper] [GitHub] [Radford et al. 2023] Robust Speech Recognition via Large-scale Weak Supervision, ICML [Paper] [GitHub] [Girdhar et al. 2023] ImageBind: One Embedding Space to Bind Them All, CVPR [Paper] [GitHub] Large Multimodal Model (LMM) [Alayrac et al. 2022] Flamingo: A Visual Language Model for Few-Shot Learning, NeurIPS [Paper] [GitHub] [Liu et al. 2023] Visual Instruction Tuning, NeurIPS [Paper] [GitHub] [Wu et al. 2024] VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks, NeurIPS [Paper] [GitHub] AI in Physical World [Driess et al. 2023] PaLM-E: An Embodied Multimodal Language Model, ICML [Paper] [GitHub] [Kim et al. 2024] OpenVLA: An Open-Source Vision-Language-Action Model, arXiv [Paper] [GitHub] [Wang et al. 2025] Magma: A Foundation Model for Multimodal AI Agents, CVPR [Paper] [GitHub] Ethics, Fairness, and AI Safety [Tao et al. 2024] When to Trust LLMs: Aligning Confidence with Response Quality, ACL [Paper] [GitHub] [Zhao et al. 2024] A Taxonomy of Challenges to Curating Fair Datasets, NeurIPS [Paper] [Li et al. 2025] T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation, CVPR [Paper] [GitHub]">

  
  

  
  
  
  
  

<script src="/vcl/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>






















  
  



  
  
  <meta name="theme-color"/>
  
  
</head>
<body
  class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
  <div id="the-top" class="absolute flex self-center">
    <a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
      href="#main-content"><span
        class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a>
  </div>
  
  
  <div style="padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px"
    class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3">
    
    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/vcl/" class="text-base font-medium text-gray-500 hover:text-gray-900">VCL Lab.</a>
            

        </nav>
        <nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12">

            
            
            
  <a href="/vcl/people"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        People
    </p>
</a>



            
            
  <a href="/vcl/research/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Research
    </p>
</a>



            
            
  <a href="/vcl/publications/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Publications
    </p>
</a>



            
            
  <a href="/vcl/courses/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Courses
    </p>
</a>



            
            
  <a href="/vcl/joining/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Joining
    </p>
</a>



            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            

        </nav>
        <div class="flex md:hidden items-center space-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            

        </div>
    </div>
    <div class="-my-2 -mr-2 md:hidden">

        <label id="menu-button" class="block">
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" style="padding-top:5px;"
                class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li id="menu-close-button">
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                    
  <li class="mt-1">
    <a href="/vcl/people"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            People
        </p>
    </a>
</li>




                    

                    
  <li class="mt-1">
    <a href="/vcl/research/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Research
        </p>
    </a>
</li>




                    

                    
  <li class="mt-1">
    <a href="/vcl/publications/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Publications
        </p>
    </a>
</li>




                    

                    
  <li class="mt-1">
    <a href="/vcl/courses/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Courses
        </p>
    </a>
</li>




                    

                    
  <li class="mt-1">
    <a href="/vcl/joining/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Joining
        </p>
    </a>
</li>




                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>





  
  <div class="relative flex flex-col grow">
    <main id="main-content" class="grow">
      



<header>
  
  <h1 class="mt-5 text-4xl font-extrabold text-neutral-900 dark:text-neutral"></h1>
  <div class="mt-1 mb-2 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
    





















<div class="flex flex-row flex-wrap items-center">
  
  
</div>


  </div>
  
  
    
    
      
      
    
  <script>
    var oid = "views_courses\/gr25624\/_index.md"
    var oid_likes = "likes_courses\/gr25624\/_index.md"
  </script>
  
  
  <script type="text/javascript" src="/vcl/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js" integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q&#43;oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script>
  
</header>

<section class="mt-0 prose flex max-w-full flex-col dark:prose-invert lg:flex-row">
  
  <div class="min-w-0 min-h-0 max-w-prose">
    <br/>


<h2 class="relative group"><span style="color: rgb(243, 123, 165); font-weight: bold;">Paper Reading List</span> 
    <div id="span-stylecolor-rgb243-123-165-font-weight-boldpaper-reading-listspan" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#span-stylecolor-rgb243-123-165-font-weight-boldpaper-reading-listspan" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<br/>
<style>
  /* Blowfish/Tailwind Typography(.prose) 기본값을 확실히 덮어쓰기 */
  .prose table.syllabus th { text-align: center !important; }
  .prose table.syllabus td { text-align: left !important; text-indent: 0 !important; padding-left: 0 !important; }
  .prose table.syllabus td * { margin: 0 !important; text-indent: 0 !important; }
  table.syllabus {
    width: 100%;
    border-collapse: collapse;
  }
  table.syllabus th, table.syllabus td {
    border: 1px solid #3a3a3a;
    padding: 8px;
    vertical-align: top;
  }
  table.syllabus th { font-weight: 600; }
  table.syllabus td.topic { text-align: center !important; }
</style>
<table class="syllabus not-prose">
  <tr>
    <th>Weekly Topic</th>
    <th>Papers for Presentation</th>
    <th>Resource</th>
  </tr>
  <!-- Deep Learning -->
  <tr>
    <td class="topic" rowspan="3">Deep Learning</td>
    <td>[Chen et al. 2020] A Simple Framework for Contrastive Learning of Visual Representations, ICML</td>
    <td align="center">
      <a href="http://proceedings.mlr.press/v119/chen20j/chen20j.pdf">[Paper]</a>
      <a href="https://github.com/google-research/simclr">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[He et al. 2020] Momentum Contrast for Unsupervised Visual Representation Learning, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf">[Paper]</a>
      <a href="https://github.com/lucidrains/byol-pytorch">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Grill et al. 2020] Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning, NeurIPS</td>
    <td align="center">
      <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf">[Paper]</a>
      <a href="https://github.com/openai/CLIP">[GitHub]</a>
    </td>
  </tr>
  <!-- Convolutional Neural Networks -->
  <tr>
    <td class="topic" rowspan="3">Convolutional Neural Networks (CNN)</td>
    <td>[Tan et al. 2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, ICML</td>
    <td align="center">
      <a href="https://arxiv.org/pdf/1905.11946">[Paper]</a>
      <a href="https://github.com/lukemelas/EfficientNet-PyTorch">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Liu et al. 2022] A ConvNet for the 2020s, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf">[Paper]</a>
      <a href="https://github.com/facebookresearch/ConvNeXt">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Woo et al. 2023] ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf">[Paper]</a>
      <a href="https://github.com/facebookresearch/ConvNeXt-V2">[GitHub]</a>
    </td>
  </tr>
  <!-- Transformer -->
  <tr>
    <td class="topic" rowspan="3">Transformer</td>
    <td>[Dosovitskiy et al. 2021] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR</td>
    <td align="center">
      <a href="https://openreview.net/pdf?id=YicbFdNTTy">[Paper]</a>
      <a href="https://github.com/google-research/vision_transformer">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Caron et al. 2021] Emerging Properties in Self-Supervised Vision Transformers, ICCV</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf">[Paper]</a>
      <a href="https://github.com/facebookresearch/dino">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[He et al. 2022] Masked Autoencoders Are Scalable Vision Learners, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf">[Paper]</a>
      <a href="https://github.com/facebookresearch/mae">[GitHub]</a>
    </td>
  </tr>
  <!-- Overview of Generative AI -->
  <tr>
    <td class="topic" rowspan="3">Overview of Generative AI</td>
    <td>[Isola et al. 2017] Image-to-Image Translation with Conditional Adversarial Networks, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf">[Paper]</a>
      <a href="https://github.com/phillipi/pix2pix">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Devlin et al. 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL</td>
    <td align="center">
      <a href="https://aclanthology.org/N19-1423.pdf">[Paper]</a>
      <a href="https://github.com/google-research/bert">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Dhariwal et al. 2021] Diffusion Models Beat GANs on Image Synthesis, NeurIPS</td>
    <td align="center">
      <a href="https://proceedings.nips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf">[Paper]</a>
      <a href="https://github.com/openai/guided-diffusion">[GitHub]</a>
    </td>
  </tr>
  <!-- Generative Models -->
  <tr>
    <td class="topic" rowspan="3">Generative Models</td>
    <td>[Mildenhall et al. 2020] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, ECCV</td>
    <td align="center">
      <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392.pdf">[Paper]</a>
      <a href="https://github.com/bmild/nerf">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Rombach et al. 2022] High-Resolution Image Synthesis with Latent Diffusion Models, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">[Paper]</a>
      <a href="https://github.com/CompVis/latent-diffusion">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Kerbl et al. 2023] 3D Gaussian Splatting for Real-time Radiance Field Rendering, SIGGRAPH</td>
    <td align="center">
      <a href="https://dl.acm.org/doi/pdf/10.1145/3592433">[Paper]</a>
      <a href="https://github.com/graphdeco-inria/gaussian-splatting">[GitHub]</a>
    </td>
  </tr>
  <!-- LLM -->
  <tr>
    <td class="topic" rowspan="3">Large Language Moudel (LLM)</td>
    <td>[Chowdhery et al. 2022] PaLM: Scaling Language Modeling with Pathways, JMLR</td>
    <td align="center">
      <a href="https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf">[Paper]</a>
      <a href="https://github.com/lucidrains/PaLM-pytorch">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Wei et al. 2022] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS</td>
    <td align="center">
      <a href="https://openreview.net/pdf?id=_VjQlMeSB_J">[Paper]</a>
    </td>
  </tr>
  <tr>
    <td>[Ouyang et al. 2022] Training Language Models to Follow Instructions with Human Feedback, NeurIPS</td>
    <td align="center">
      <a href="https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf">[Paper]</a>
      <a href="https://github.com/openai/following-instructions-human-feedback">[GitHub]</a>
    </td>
  </tr>
  <!-- Multimodal Learning -->
  <tr>
    <td class="topic" rowspan="3">Multimodal Learning</td>
    <td>[Radford et al. 2021] Learning Transferable Visual Models from Natural Language Supervision, ICML</td>
    <td align="center">
      <a href="https://proceedings.mlr.press/v139/radford21a/radford21a.pdf">[Paper]</a>
      <a href="https://github.com/openai/CLIP">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Radford et al. 2023] Robust Speech Recognition via Large-scale Weak Supervision, ICML</td>
    <td align="center">
      <a href="https://cdn.openai.com/papers/whisper.pdf">[Paper]</a>
      <a href="https://github.com/openai/whisper">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Girdhar et al. 2023] ImageBind: One Embedding Space to Bind Them All, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf">[Paper]</a>
      <a href="https://github.com/facebookresearch/ImageBind">[GitHub]</a>
    </td>
  </tr>
  <!-- LMM -->
  <tr>
    <td class="topic" rowspan="3">Large Multimodal Model (LMM)</td>
    <td>[Alayrac et al. 2022] Flamingo: A Visual Language Model for Few-Shot Learning, NeurIPS</td>
    <td align="center">
      <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf">[Paper]</a>
      <a href="https://github.com/lucidrains/flamingo-pytorch">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Liu et al. 2023] Visual Instruction Tuning, NeurIPS</td>
    <td align="center">
      <a href="https://papers.nips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf">[Paper]</a>
      <a href="https://github.com/haotian-liu/LLaVA">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Wu et al. 2024] VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks, NeurIPS</td>
    <td align="center">
      <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/81a60d18e010b27b36cd465c6604b915-Paper-Conference.pdf">[Paper]</a>
      <a href="https://github.com/OpenGVLab/VisionLLM">[GitHub]</a>
    </td>
  </tr>
  <!-- AI in Physical World -->
  <tr>
    <td class="topic" rowspan="3">AI in Physical World</td>
    <td>[Driess et al. 2023] PaLM-E: An Embodied Multimodal Language Model, ICML</td>
    <td align="center">
      <a href="https://proceedings.mlr.press/v202/driess23a/driess23a.pdf">[Paper]</a>
      <a href="https://github.com/kyegomez/PALM-E">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Kim et al. 2024] OpenVLA: An Open-Source Vision-Language-Action Model, arXiv</td>
    <td align="center">
      <a href="https://arxiv.org/pdf/2406.09246">[Paper]</a>
      <a href="https://github.com/openvla/openvla">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Wang et al. 2025] Magma: A Foundation Model for Multimodal AI Agents, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Magma_A_Foundation_Model_for_Multimodal_AI_Agents_CVPR_2025_paper.pdf">[Paper]</a>
      <a href="https://github.com/microsoft/Magma">[GitHub]</a>
    </td>
  </tr>
  <!-- Ethics, Fairness, and AI Safety -->
  <tr>
    <td class="topic" rowspan="3">Ethics, Fairness, and AI Safety</td>
    <td>[Tao et al. 2024] When to Trust LLMs: Aligning Confidence with Response Quality, ACL</td>
    <td align="center">
      <a href="https://aclanthology.org/2024.findings-acl.357.pdf">[Paper]</a>
      <a href="https://github.com/TaoShuchang/CONQORD">[GitHub]</a>
    </td>
  </tr>
  <tr>
    <td>[Zhao et al. 2024] A Taxonomy of Challenges to Curating Fair Datasets, NeurIPS</td>
    <td align="center">
      <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/b142e78db191e19b17e60c1425a28b52-Paper-Datasets_and_Benchmarks_Track.pdf">[Paper]</a>
    </td>
  </tr>
  <tr>
    <td>[Li et al. 2025] T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation, CVPR</td>
    <td align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_T2ISafety_Benchmark_for_Assessing_Fairness_Toxicity_and_Privacy_in_Image_CVPR_2025_paper.pdf">[Paper]</a>
      <a href="https://github.com/adwardlee/t2i_safety">[GitHub]</a>
    </td>
  </tr>
</table>
<style>
  table.syllabus {
    width: 100%;
    border-collapse: collapse;
    font-size: 0.9em; /* 글씨 크기 축소 */
  }
  table.syllabus th, 
  table.syllabus td {
    border: 1px solid #3a3a3a;
    padding: 6px;
    vertical-align: top;
  }

  /* 제목은 가운데 */
  table.syllabus th {
    text-align: center;
    font-weight: 600;
  }

  /* Weekly Topic(1열), Resource(3열)은 항상 가운데 */
  table.syllabus td:nth-child(1),
  table.syllabus td:nth-child(3) {
    text-align: center;
  }

  /* 두번째 열은 왼쪽 정렬 */
  table.syllabus td:nth-child(2) {
    text-align: left;
  }

  /* 들여쓰기 제거 */
  table.syllabus td, 
  table.syllabus td * {
    text-indent: 0 !important;
    margin: 0 !important;
    padding-left: 0 !important;
  }

  /* 링크 색상 파란색 계열 */
  table.syllabus a {
    color: #3399ff;
    text-decoration: none; /* 밑줄 제거하려면 */
  }
  table.syllabus a:hover {
    text-decoration: underline; /* 마우스 올리면 밑줄 */
  }
</style>
  </div>
</section>




      <div id="top-scroller" class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0">
  <a href="#the-top"
    class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top" title="Scroll to top">
    &uarr;
  </a>
</div>
    </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
  
  <div class="flex items-center justify-between">

    
    
    <p class="text-sm text-neutral-500 dark:text-neutral-400">
      &copy;
      2025
      
    </p>
    

    
    
    
    <p class="text-xs text-neutral-500 dark:text-neutral-400">
      VCL Lab @ School of ECE, Chonnam National University</a>
    </p>
    

  </div>
  <script>
    
    mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
      margin: 24,
      background: 'rgba(0,0,0,0.5)',
      scrollOffset: 0,
    })
    
  </script>
  
  
  <script type="text/javascript" src="/vcl/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js" integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="https://hikimece.github.io/vcl/"
  style="z-index:500"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)"
      >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

  </div>
</body>

</html>


